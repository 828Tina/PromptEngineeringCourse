# æç¤ºè¯å·¥ç¨‹æ•™ç¨‹

æç¤ºè¯å·¥ç¨‹ï¼ˆPrompt Engingeringï¼‰ï¼Œä¹Ÿè¢«ç§°ä¸ºä¸Šä¸‹æ–‡æç¤ºï¼ˆIn-Context Promptingï¼‰ï¼ŒæŒ‡çš„æ˜¯é€šè¿‡ç»“æ„åŒ–æ–‡æœ¬çš„æ–¹æ³•æ¥å®Œå–„æç¤ºè¯ï¼ˆPromptï¼‰ï¼Œä»è€Œå¼•å¯¼å¤§æ¨¡å‹ç”Ÿæˆæ›´ç¬¦åˆé¢„æœŸçš„è¾“å‡ºç»“æœçš„æŠ€æœ¯ã€‚ç®€å•ç‚¹è¯´å°±æ˜¯â€œç”¨æ›´èªæ˜çš„æé—®æ–¹å¼ï¼Œè®©AIæ›´å¥½åœ°ç†è§£å¹¶å®Œæˆä»»åŠ¡â€ã€‚é€šè¿‡æç¤ºè¯å·¥ç¨‹å¯ä»¥åœ¨ä¸é€šè¿‡è®­ç»ƒæ›´æ–°æ¨¡å‹æƒé‡çš„æƒ…å†µä¸‹ï¼Œè®©å¤§æ¨¡å‹å®Œæˆä¸åŒç±»å‹çš„ä»»åŠ¡ã€‚æç¤ºè¯å·¥ç¨‹çš„æ•ˆæœåœ¨ä¸åŒçš„æ¨¡å‹ä¸­å¯èƒ½ä¼šæœ‰å¾ˆå¤§çš„å·®å¼‚ï¼Œå› æ­¤éœ€è¦å¤§é‡çš„å®éªŒå’Œæ¢ç´¢ã€‚æŒæ¡äº†æç¤ºå·¥ç¨‹ç›¸å…³æŠ€èƒ½å°†æœ‰åŠ©äºç”¨æˆ·æ›´å¥½åœ°äº†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›å’Œå±€é™æ€§ã€‚

## ğŸ’¡å‰è¨€

è¿™ç§æ— éœ€å¾®è°ƒå°±èƒ½è§£å†³é—®é¢˜çš„ç‰¹æ€§ï¼Œè®©æç¤ºè¯åœ¨å®é™…åº”ç”¨ä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚è¦çŸ¥é“ï¼Œä¸æ˜¯æ‰€æœ‰åœºæ™¯éƒ½æœ‰è¶³å¤Ÿçš„æ ‡æ³¨æ•°æ®æ¥åšå¾®è°ƒï¼Œä¹Ÿä¸æ˜¯æ‰€æœ‰å›¢é˜Ÿéƒ½æœ‰èƒ½åŠ›æ‰¿æ‹…å¾®è°ƒçš„æˆæœ¬ã€‚è¿™æ—¶å€™ï¼Œä¸€å¥ç²¾å‡†çš„æç¤ºè¯å°±èƒ½è®©å¤§æ¨¡å‹åœ¨é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬çš„æƒ…å†µä¸‹å®Œæˆä»»åŠ¡ï¼Œå¤§å¤§é™ä½äº†ä½¿ç”¨é—¨æ§›ã€‚ä½†æˆ‘ä»¬ä¹Ÿè¦æ¸…æ¥šï¼Œæç¤ºè¯çš„æ•ˆæœå’Œæ¨¡å‹æœ¬èº«çš„èƒ½åŠ›ç´§å¯†ç›¸å…³ã€‚åªæœ‰å½“æ¨¡å‹å‚æ•°é‡è¾¾åˆ°ä¸€å®šè§„æ¨¡ï¼Œæ¶Œç°å‡ºè¶³å¤Ÿå¼ºçš„ç†è§£å’Œæ¨ç†èƒ½åŠ›æ—¶ï¼Œæç¤ºè¯æ‰èƒ½å‘æŒ¥æœ€å¤§ä½œç”¨ã€‚è¦æ˜¯æ¨¡å‹æœ¬èº« â€œåº•å­è–„â€ï¼Œå“ªæ€•æç¤ºè¯è®¾è®¡å¾—å†ç²¾å·§ï¼Œä¹Ÿå¾ˆéš¾å¾—åˆ°ç†æƒ³çš„ç»“æœã€‚

## ğŸš€ç¯å¢ƒå®‰è£…ä¸å¹³å°å‡†å¤‡

### âš™ï¸ç¯å¢ƒå®‰è£…

1. python>=3.9
2. pytorchï¼šåœ¨å®˜ç½‘å®‰è£…æœ€æ–°ç‰ˆæœ¬å³å¯ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯2.5ç‰ˆæœ¬bugè¾ƒå¤šï¼Œä¸å»ºè®®ä½¿ç”¨
3. å…¶ä»–çš„pythonåº“ï¼štransformersã€modelscopeã€vllmç­‰

```python
# åˆ›å»ºç¯å¢ƒ
conda create -n test python=3.10

# pytorch
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# å…¶ä»–
pip install -U transformers modelscope
```

### ğŸ”å¹³å°é€‰æ‹©

æ¨¡å‹å¯ä»¥ä¿å­˜åˆ°æœ¬åœ°è¿è¡Œï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨äº‘å¹³å°ï¼Œè°ƒç”¨APIä½¿ç”¨å¤§æ¨¡å‹ï¼Œä¸‹é¢æˆ‘ä»¬ç®€å•ä¸¾äº†å‡ ä¸ªä¾‹å­ï¼Œæ–¹ä¾¿å¤§å®¶å¿«é€Ÿä¸Šæ‰‹å¤§æ¨¡å‹çš„ä½¿ç”¨ã€‚

> HuggingFaceå’ŒvLLMçš„æ–¹æ³•éƒ½éœ€è¦æœ¬åœ°ä¿å­˜æ¨¡å‹ï¼ŒAPIçš„æ–¹å¼ä¸ç”¨ã€‚

**HuggingFace**

å¦‚æœå°†æ¨¡å‹ä¿å­˜åœ¨***æœ¬åœ°***ï¼Œå¯ä»¥ä½¿ç”¨Huggingfaceæä¾›çš„æ¨ç†ä»£ç ï¼Œåœ¨åç»­çš„ä¸¾ä¾‹ä¸­ï¼Œå¦‚æœæ²¡æœ‰ç‰¹æ®Šè¯´æ˜ï¼Œæˆ‘ä»¬éƒ½ä¼šä½¿ç”¨Qwenç³»åˆ—æ¨¡å‹ä½œä¸ºåŸºçº¿æ¨¡å‹ï¼Œå…³äºæ¨ç†ä»£ç ï¼Œæˆ‘ä»¬ä½¿ç”¨Qwenå®˜ç½‘æä¾›çš„[æ¨ç†ä»£ç ](https://www.modelscope.cn/models/Qwen/Qwen2.5-7B-Instruct)

<details>
  <summary>å®Œæ•´æ¨ç†ä»£ç </summary>
  
```python
### åŠ è½½æ¨¡å‹
from modelscope import AutoModelForCausalLM, AutoTokenizer

model_name_or_path = 'Qwen/Qwen2.5-3B'  # æ›¿æ¢ä¸ºä½ ä¸‹è½½çš„æ¨¡å‹è·¯å¾„
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
model = AutoModelForCausalLM.from_pretrained(model_name_or_path,device_map='auto', torch_dtype='auto')

### æç¤ºè¯
prompt = "Hello, Who are you?"

### æ¨ç†ä»£ç 
messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```

</details>


**vLLM**

`vLLM`æ˜¯ä¼¯å…‹åˆ©å¤§å­¦LMSYSç»„ç»‡å¼€æºçš„å¤§è¯­è¨€æ¨¡å‹é«˜é€Ÿæ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨æå¤§åœ°æå‡å®æ—¶åœºæ™¯ä¸‹çš„è¯­è¨€æ¨¡å‹æœåŠ¡çš„ååä¸å†…å­˜ä½¿ç”¨æ•ˆç‡ã€‚vLLMæ˜¯ä¸€ä¸ªå¿«é€Ÿä¸”æ˜“äºä½¿ç”¨çš„åº“ï¼Œç”¨äº LLM æ¨ç†å’ŒæœåŠ¡ï¼Œå¯ä»¥å’ŒHuggingFace æ— ç¼é›†æˆã€‚vLLMåˆ©ç”¨äº†å…¨æ–°çš„æ³¨æ„åŠ›ç®—æ³•ã€ŒPagedAttentionã€ï¼Œæœ‰æ•ˆåœ°ç®¡ç†æ³¨æ„åŠ›é”®å’Œå€¼ã€‚

`vllmåœ¨ååé‡æ–¹é¢ï¼ŒvLLMçš„æ€§èƒ½æ¯”HuggingFace Transformers(HF)é«˜å‡º 24 å€ï¼Œæ–‡æœ¬ç”Ÿæˆæ¨ç†ï¼ˆTGIï¼‰é«˜å‡º3.5å€ã€‚`

ç®€å•ç‚¹è¯´å°±æ˜¯vllmæ¡†æ¶çš„æ¨ç†é€Ÿåº¦å¾ˆå¿«ï¼Œä½†æ˜¯æ˜¾å­˜å ç”¨è¾ƒé«˜ï¼ŒåŒæ ·çš„3Bæ¨¡å‹ï¼Œæœ¬åœ°æ¨ç†å¯èƒ½åªéœ€è¦15GBå·¦å³ï¼Œè€Œvllmæ¡†æ¶åˆ™éœ€è¦37GBï¼Œå› æ­¤å¦‚æœç¡¬ä»¶èµ„æºä¸è¶³ï¼Œvllmå¹¶ä¸æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©ã€‚

<details>
  <summary>å®Œæ•´æµç¨‹</summary>

---

1. åœ¨ç¯å¢ƒé‡Œå®‰è£…vllmåº“

   
```python
pip install -U vllm
```

2. å¼€å¯ä¸€ä¸ªç»ˆç«¯é¡µé¢ï¼Œè¿è¡Œä¸‹é¢çš„ä»£ç 

```python
vllm serve /your/path/of/model
```

3. å¼€å¯æ–°çš„ç»ˆç«¯é¡µé¢è¿è¡Œå„ä¸ªä»£ç 

åœ¨æ–°çš„ç»ˆç«¯é¡µé¢ï¼Œæˆ‘ä»¬å°±å¯ä»¥è·‘æˆ‘ä»¬å¯¹åº”çš„æœåŠ¡äº†ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤§æ¨¡å‹æ¥è¿›è¡Œæ¨ç†çš„æ—¶å€™å¯ä»¥ä½¿ç”¨[openaiçš„promptçš„APIçš„æ¥å£](https://openai.apifox.cn/api-55352401)ï¼Œè¾“å…¥æ¥å£å‚è€ƒç»™å‡ºçš„æ–‡æ¡£å³å¯ï¼Œè®²èµ·æ¥æ¯”è¾ƒæŠ½è±¡ï¼Œæˆ‘ä»¬çœ‹ä¸‹ä»£ç ä¾‹å­ï¼š

*æ¨¡å‹ç”Ÿæˆå›ç­”çš„ä»£ç  ï¼š*

```python
results = utils.openai_completion(
            prompts=batch_inputs,
            model_name=model_name,
            batch_size=request_batch_size,
            decoding_args=decoding_args,
            # logit_bias={"50256": -100},  # prevent the <|endoftext|> token from being generated
        )

```



*å¯¹åº”çš„å·¥å…·utilsçš„ openai_completionå‡½æ•°ï¼š*

```python
def openai_completion(
    prompts: Union[str, Sequence[str], Sequence[dict[str, str]], dict[str, str]],
    decoding_args: OpenAIDecodingArguments,
    model_name="text-davinci-003",
    sleep_time=2,
    batch_size=1,
    max_instances=sys.maxsize,
    max_batches=sys.maxsize,
    return_text=False,
    **decoding_kwargs,
):
â€¦â€¦
â€¦â€¦
    completion_batch = client.completions.create(
                prompt=prompt_batch, **shared_kwargs
            )
    choices = completion_batch.choices
    
â€¦â€¦
â€¦â€¦

```


`client.completions.create`å…¶å®ä½¿ç”¨çš„å°±æ˜¯openaiåº“ä¸­çš„æ¨ç†é—®ç­”çš„å‡½æ•°ã€‚

ç»è¿‡è¿™äº›æ­¥éª¤åï¼Œæˆ‘ä»¬åœ¨è¿è¡Œæ¨ç†æœåŠ¡çš„æ—¶å€™å°±èƒ½å……åˆ†åˆ©ç”¨GPUèµ„æºï¼Œé«˜æ•ˆå®Œæˆå„é¡¹æ¨ç†ä»»åŠ¡ã€‚


</details>

**API**

å¤§æ¨¡å‹çš„ API è°ƒç”¨ï¼Œç®€å•æ¥è¯´ï¼Œæ˜¯æŒ‡å¼€å‘è€…æˆ–ç”¨æˆ·é€šè¿‡åº”ç”¨ç¨‹åºç¼–ç¨‹æ¥å£ï¼ˆAPIï¼‰ ä¸å¤§æ¨¡å‹è¿›è¡Œäº¤äº’ï¼Œä»è€Œåˆ©ç”¨å¤§æ¨¡å‹çš„èƒ½åŠ›å®Œæˆç‰¹å®šä»»åŠ¡çš„è¿›ç¨‹ï¼Œè¿™äº›å¤§æ¨¡å‹æ— éœ€éƒ¨ç½²åˆ°æœ¬åœ°ï¼Œä½ ä½¿ç”¨çš„èµ„æºå…¶å®æ˜¯è¿™äº›å‚å•†æä¾›çš„æœåŠ¡é›†ç¾¤ï¼Œä¸ä»…æ¨ç†é€Ÿåº¦å¿«ï¼Œè€Œä¸”å¹¶ä¸å ç”¨æ˜¾å­˜ï¼Œä¸è¿‡æ¯ä¸ªæ¨¡å‹ä¼šæœ‰è´¹ç”¨çš„æ¶ˆè€—ï¼Œä¸ä¸€å®šèƒ½å…è´¹ä½¿ç”¨ã€‚

æœ¬æ¬¡æ•™ç¨‹æˆ‘ä»¬å‚è€ƒäº†ä¸¤ä¸ªäº‘å¹³å°ï¼Œé“¾æ¥å¦‚ä¸‹ï¼š

1. [ç¡…åŸºæµåŠ¨](https://cloud.siliconflow.cn/sft-d1n0sv33jrms738gmgpg/models)
2. [é˜¿é‡Œäº‘ç™¾ç‚¼](https://bailian.console.aliyun.com/?spm=5176.12818093_47.resourceCenter.1.223c2cc96V9eQn&tab=api#/api/?type=model&url=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F2712576.html)



## ğŸ“šè¯¾ç¨‹ç›®å½•


| æ•™ç¨‹ç« èŠ‚   | æ ¸å¿ƒå†…å®¹ |  
|:--------|:------|
| [1.å‰ç½®å‡†å¤‡](./1.preparatory_work_in_advance)   | æ¨¡å‹å®‰è£…ã€å¹³å°é€‰æ‹©ã€æ¨¡å‹é€‰æ‹©(base,instruct,reasoning)   |
| [2.æç¤ºè¯æ’°å†™æŠ€å·§](./2.tips_for_prompt)   | æç¤ºè¯ç»“æ„ã€æç¤ºè¯è¦ç´ ã€æ€ç»´é“¾ç­‰   |
| [3.å¸¸è§ä»»åŠ¡ç¤ºä¾‹](./3.common_task_examples)   | æ–‡æœ¬æ‘˜è¦ã€æ•°å­¦è®¡ç®—ã€å¤šè½®å¯¹è¯ç­‰   | 
| [4.å¤šæ¨¡æ€æç¤ºè¯](./4.multimodal_prompt)   | å¤šæ¨¡æ€æç¤ºè¯ä½¿ç”¨ç¤ºä¾‹   | 
| [5.åˆæˆæ•°æ®](./5.synthetic_data)   | é¢„è®­ç»ƒã€å¾®è°ƒã€æ¨ç†æ•°æ®é›†åˆæˆä»£ç å®è·µ   | 
| [6.RAG](./6.RAG)   | RAGå®è·µ   | 
| [7.Agent](./7.Agent)   | å‡½æ•°è°ƒç”¨ã€MCPå®è·µã€AgentsåŸç†   | 
| [8.æç¤ºè¯ä¸ä¸Šä¸‹æ–‡å·¥ç¨‹](xxx)   | æç¤ºè¯å·¥ç¨‹ä¸ä¸Šä¸‹æ–‡å·¥ç¨‹åŒºåˆ«  | 


---

### ğŸŒŸæ ¸å¿ƒè´¡çŒ®

- [æé¦¨é›¨](https://github.com/828Tina) ï¼ˆæƒ…æ„Ÿæœºå™¨ï¼ˆåŒ—äº¬ï¼‰ç§‘æŠ€æœ‰é™å…¬å¸æŠ€æœ¯å‘˜-è¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦ï¼‰







